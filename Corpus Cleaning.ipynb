{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "76f1b691",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Will\n",
      "[nltk_data]     Boyd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Will\n",
      "[nltk_data]     Boyd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b48406b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       feature  sentiment  label\n",
      "0            !    0.50000      0\n",
      "1          ! '    0.52778      0\n",
      "2         ! ''    0.50000      0\n",
      "3       ! Alas    0.44444      0\n",
      "4  ! Brilliant    0.86111      0\n"
     ]
    }
   ],
   "source": [
    "# Load in the dictionary dataset into a pandas df\n",
    "data_path = os.path.join(os.getcwd(), 'data', 'dictionary.txt')\n",
    "dic = pd.read_csv(data_path, sep='|', header=None)\n",
    "\n",
    "# ...and do the same with the sentiment_labels data\n",
    "data_path = os.path.join(os.getcwd(), 'data', 'sentiment_labels.txt')\n",
    "cents = pd.read_csv(data_path, sep='|')\n",
    "\n",
    "dic = dic.rename(columns={0: \"feature\", 1: \"ID\"})# We rename the columns\n",
    "# dic = dic.rename(index={range(239232)})\n",
    "dic = dic.sort_values(by=\"ID\") # We change the order of the rows to be sorted by ID number\n",
    "movie_data = dic[[\"ID\", \"feature\"]] # We change the order of the columns and change the name of the the df\n",
    "\n",
    "# We make an array of the sentiments (already in the right order) and add it to our df\n",
    "y = np.array(cents.iloc[:, -1])\n",
    "movie_data.insert(2, \"sentiment\", y)\n",
    "\n",
    "phrase_data = movie_data.sort_index()\n",
    "phrase_data.drop(\"ID\",axis=1,inplace=True)\n",
    "phrase_data.insert(2,\"label\",0)\n",
    "\n",
    "print(phrase_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6a7383d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36093</th>\n",
       "      <td>Its compelling mix of trial movie , escape mov...</td>\n",
       "      <td>0.83333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142154</th>\n",
       "      <td>it 's a bargain-basement European pickup .</td>\n",
       "      <td>0.31944</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62476</th>\n",
       "      <td>a ` very sneaky ' butler who excels in the art...</td>\n",
       "      <td>0.55556</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23264</th>\n",
       "      <td>Charming and funny ( but ultimately silly ) mo...</td>\n",
       "      <td>0.56944</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73120</th>\n",
       "      <td>acrid test</td>\n",
       "      <td>0.47222</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  feature  sentiment  label\n",
       "36093   Its compelling mix of trial movie , escape mov...    0.83333      0\n",
       "142154         it 's a bargain-basement European pickup .    0.31944      0\n",
       "62476   a ` very sneaky ' butler who excels in the art...    0.55556      0\n",
       "23264   Charming and funny ( but ultimately silly ) mo...    0.56944      0\n",
       "73120                                          acrid test    0.47222      0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### We create a smaller subsample of the dataset - to speed up the computation when working on our code\n",
    "\n",
    "# specify a smaller number of reviews\n",
    "small_N = 2390\n",
    "\n",
    "# choose small_N random and distinct integers between 0 and 239231\n",
    "rand = random.sample(range(239231), small_N)\n",
    "\n",
    "# find these indices in the original dictionary - and make a new array of them\n",
    "rand_sample = phrase_data.iloc[rand]\n",
    "\n",
    "rand_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f85ee551",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = rand_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "01939b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "### add correct labels based on sentiment column (uses qualities of numpy for efficiency)\n",
    "\n",
    "np_phrase = np.array(phrases) # make it a numpy array\n",
    "\n",
    "# create a series of boolean masks\n",
    "vpos = (0.8 < np_phrase[:, 1]).astype(int)\n",
    "pos =  (0.6 < np_phrase[:, 1]).astype(int)\n",
    "ntrl = (0.4 < np_phrase[:, 1]).astype(int)\n",
    "neg = (0.2 < np_phrase[:, 1]).astype(int)\n",
    "vneg = (0 <= np_phrase[:, 1]).astype(int)\n",
    "\n",
    "# add the masks together to get the correct label numbers for each review based on sentiment value\n",
    "labels = vneg + neg + ntrl + pos + vpos - 1\n",
    "\n",
    "# make a binary label class\n",
    "binary_labels = (0.5 >= np_phrase[:, 1]).astype(int)\n",
    "\n",
    "# update the array with our new values\n",
    "np_phrase[:, 2] = labels\n",
    "# np_phrase[:, 2] = binary_labels\n",
    "\n",
    "# change back to a pandas\n",
    "phrases = pd.DataFrame(np_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bfaf795e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7586460632818248\n"
     ]
    }
   ],
   "source": [
    "print(len(binary_labels[binary_labels == 0])/len(binary_labels[binary_labels == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a0267b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0  2\n",
      "0  [compelling, mix, trial, movie, escape, movie,...  4\n",
      "1              [bargain, basement, european, pickup]  1\n",
      "2  [sneaky, butler, excels, art, impossible, disa...  2\n",
      "3        [charming, funny, ultimately, silly, movie]  2\n",
      "4                                      [acrid, test]  2\n",
      "(2341, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Will Boyd\\AppData\\Local\\Temp\\ipykernel_31108\\280668464.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.drop(\"index\",axis=1,inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Feature importance\n",
    "\n",
    "filler_words = set(stopwords.words('english'))\n",
    "lemmatize = WordNetLemmatizer()\n",
    "\n",
    "#values =[[0, 0.2], [0.2, 0.4], [0.4, 0.6], [0.6, 0.8], [0.8, 1.0]]\n",
    "labels = ['very negative', 'negative', 'neutral', 'positive', 'very positive']\n",
    "to_drop = []\n",
    "\n",
    "phrases_list = list(phrases.iloc[:, 0])\n",
    "\n",
    "for i in range(len(phrases_list)):\n",
    "    # 'clean' phrases: remove numbers, punctuation and filler words\n",
    "    phrase = phrases_list[i]\n",
    "    phrase = re.sub(r'[^\\w]', \" \", phrase) #remove all special characters \n",
    "    cleaned = re.sub(r'[\\d]', \" \", phrase)  #remove all numbers\n",
    "\n",
    "    if (cleaned.replace(\" \",\"\")==\"\"):\n",
    "        to_drop.append(i)\n",
    "        cleaned=\"\"\n",
    "    else:\n",
    "        cleaned = word_tokenize(cleaned.lower()) #tokenise for bag of words\n",
    "        cleaned = [w for w in cleaned if w not in filler_words] # #remove all filler words\n",
    "        cleaned = [lemmatize.lemmatize(word) for word in cleaned]\n",
    "    \n",
    "    phrases_list[i] = cleaned\n",
    "    \n",
    "phrases.iloc[:, 0] = phrases_list\n",
    "\n",
    "# remove unnecessary data\n",
    "phrases.drop(1,axis=1,inplace=True) \n",
    "phrases.drop(to_drop,axis=0,inplace=True)\n",
    "df = phrases[~phrases.astype(str).duplicated()]\n",
    "df.reset_index(inplace=True)\n",
    "df.drop(\"index\",axis=1,inplace=True)\n",
    "print(df.head())\n",
    "print(df.shape)\n",
    "\n",
    "#save this as file\n",
    "df.to_pickle(\"clean_doc.pkl\", protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "114a3df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "### we are going to train our embedding model on the original review corpus\n",
    "### in the hope to capture more semantic information\n",
    "lemmatize = WordNetLemmatizer()\n",
    "# opening the file in read mode and reading the file\n",
    "orig_snip = open(\"original_rt_snippets.txt\")\n",
    "orig_snip = orig_snip.read()\n",
    "orig_snip = orig_snip.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ea851d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looping over each review\n",
    "for i in range(len(orig_snip)):\n",
    "    \n",
    "    orig_snip[i] = orig_snip[i].lower() # lower case\n",
    "    \n",
    "    orig_snip[i] = re.sub(r'[^\\w]', \" \", orig_snip[i]) # remove all special characters\n",
    "    orig_snip[i] = re.sub(r'[\\d]', \" \", orig_snip[i]) # ...and numbers\n",
    "    \n",
    "    orig_snip[i] = nltk.word_tokenize(orig_snip[i]) # make each word an individual string, thus each review is a sublist\n",
    "    \n",
    "    orig_snip[i] = [word for word in orig_snip[i] if word not in stopwords.words(\"english\")] # removing stop-words\n",
    "    orig_snip[i] = [lemmatize.lemmatize(word) for word in orig_snip[i]] # lemmatizing (converting to the grammatical root)\n",
    "\n",
    "with open(\"clean_snippets.pkl\", \"wb\") as file:\n",
    "    pickle.dump(orig_snip, file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188f8f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_snippets = pd.read_pickle(\"clean_snippets.pkl\").iloc[:-1, :][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66821b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clean_snippets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
